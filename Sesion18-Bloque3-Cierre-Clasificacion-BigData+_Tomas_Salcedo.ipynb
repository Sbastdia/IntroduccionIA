{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496631ea",
   "metadata": {
    "id": "496631ea"
   },
   "source": [
    "# Módulo 1: Análisis de datos en el ecosistema Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14cd3f",
   "metadata": {
    "id": "3a14cd3f"
   },
   "source": [
    "### Sesión (18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd93e7",
   "metadata": {
    "id": "6acd93e7"
   },
   "source": [
    "**30/01/2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaae337",
   "metadata": {
    "id": "fbaae337"
   },
   "source": [
    "## Aplicar un caso de uso (*Clasificación*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a1419",
   "metadata": {
    "id": "1d4a1419"
   },
   "outputs": [],
   "source": [
    "# importamos las librerías necesarias \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d67e37",
   "metadata": {
    "id": "14d67e37"
   },
   "outputs": [],
   "source": [
    "# Modificamos los parámetros de los gráficos en matplotlib\n",
    "from matplotlib.pyplot import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6 # el primer dígito es el ancho y el segundo el alto\n",
    "rcParams[\"font.weight\"] = \"bold\"\n",
    "rcParams[\"font.size\"] = 10\n",
    "rcParams[\"axes.labelweight\"] = \"bold\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7c738",
   "metadata": {
    "id": "a2a7c738"
   },
   "source": [
    "### Dataset de cubierta forestal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa35de9",
   "metadata": {
    "id": "5fa35de9"
   },
   "source": [
    "**[Forest Covertype data](https://archive.ics.uci.edu/ml/datasets/Covertype)** es un conjunto de datos cargado en la librería _sklearn_ que permite realizar un ejercicio tipo problemas de **clasificación**. El objetivo de este dataset es **estudiar las variables cartográficas** para poder **predecir el tipo de cubierta forestal**. El tipo real de cubierta forestal para una observación (celda de 30 x 30 metros) se ha determinado a partir de los datos del **Servicio Forestal de EE.UU. (USFS)**. \n",
    "\n",
    "Los datos están en forma **cruda** (sin escalar) y contienen columnas binarias (0 o 1) de datos para variables independientes cualitativas (áreas silvestres y tipos de suelo).\n",
    "\n",
    "Estas áreas de estudio representan **bosques con mínimas perturbaciones causadas por el hombre**, por lo que los tipos de cubierta forestal existentes son más el **resultado de procesos ecológicos**, que de prácticas de gestión forestal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cf699",
   "metadata": {
    "id": "df9cf699"
   },
   "source": [
    "### Análisis Exploratorio Inicial, Tratamiento y Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe776ee0",
   "metadata": {
    "id": "fe776ee0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "# Construimos un dataframe con los datos medidos de la cubierta forestal\n",
    "dataset_cub = pd.DataFrame(fetch_covtype()[\"data\"], columns=fetch_covtype()[\"feature_names\"])\n",
    "\n",
    "# Añadimos la variable objetivo\n",
    "dataset_cub['target'] = fetch_covtype()[\"target\"]\n",
    "\n",
    "dataset_cub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865ed5a",
   "metadata": {
    "id": "c865ed5a"
   },
   "source": [
    "Vamos a sacar un dataset mucho más pequeño para **simplificar los cálculos** y **reducir el tiempo de computación** de los algoritmos. Una primera idea para extraer un subconjunto podría ser quedarnos por ejemplo con los **_100.000_ primeros registros** de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d4737",
   "metadata": {
    "id": "f10d4737"
   },
   "outputs": [],
   "source": [
    "dataset_cub[0:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a74f10",
   "metadata": {
    "id": "b2a74f10"
   },
   "source": [
    "El posible peligro de este enfoque es que **no obtengamos un subconjunto muy representativo**. Realizamos una consulta sobre una de las variables del datset para ver si estadísticamente tienen las mismas características o no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbe207",
   "metadata": {
    "id": "88cbe207"
   },
   "outputs": [],
   "source": [
    "dataset_cub['Elevation'].describe()[['count', 'mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9653c0",
   "metadata": {
    "id": "9e9653c0"
   },
   "outputs": [],
   "source": [
    "dataset_cub[0:100000]['Elevation'].describe()[['count', 'mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c46d7f",
   "metadata": {
    "id": "08c46d7f"
   },
   "outputs": [],
   "source": [
    "# Comparar la distribución de la variable \"Elevation\" entre los dos Dataframes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,9))\n",
    "sns.histplot(dataset_cub['Elevation'], bins=200, ax=axes[0])\n",
    "sns.histplot(dataset_cub[0:100000]['Elevation'], bins=200, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fd6f9",
   "metadata": {
    "id": "226fd6f9"
   },
   "source": [
    "Se puede observar que la variable analizada no tiene la misma representación en el subconjunto de los _100.000_ primeros registros del dataset. Otra opción para conseguir un trozo de los datos originales, sería aplicar la técnica de **remuestro aleatorio (_random resampling_)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bf8ed",
   "metadata": {
    "id": "430bf8ed"
   },
   "source": [
    "Vamos a hacer un remuestreo aleatorio para **quedarnos aproximadamente con el 17% de los datos** usando el método **[sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)** sobre los _DataFrames_ de la librería _pandas_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880befd7",
   "metadata": {
    "id": "880befd7"
   },
   "outputs": [],
   "source": [
    "100/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZHVM7fdPLr4",
   "metadata": {
    "id": "RZHVM7fdPLr4"
   },
   "outputs": [],
   "source": [
    "df_cub = dataset_cub.sample(frac=1/5.81012, random_state=222).reset_index(drop=True)\n",
    "df_cub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd3683",
   "metadata": {
    "id": "3ccd3683"
   },
   "outputs": [],
   "source": [
    "# Consultamos las principales esdadísticas de la variable analizada anteriormente\n",
    "df_cub['Elevation'].describe()[['count', 'mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5cb55",
   "metadata": {
    "id": "08e5cb55"
   },
   "outputs": [],
   "source": [
    "# Comparar la distribución de la variable \"Elevation\" entre el DataFrame original y el subconjunto obtenido mediante el remuestreo aleatorio\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,9))\n",
    "sns.histplot(dataset_cub['Elevation'], bins=200, ax=axes[0])\n",
    "sns.histplot(df_cub['Elevation'], bins=200, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8dcaf",
   "metadata": {
    "id": "dde8dcaf"
   },
   "source": [
    "Podemos ver que este último subconjunto, a pesar de tener solamente 17% de los datos, **sí que incluye un set de observaciones muy representativas** respecto al dataset original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c1eb9",
   "metadata": {
    "id": "f17c1eb9"
   },
   "outputs": [],
   "source": [
    "# Comparar la distribución de la variable objetivo entre los dos Dataframes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,5))\n",
    "sns.histplot(dataset_cub['target'], bins=20, ax=axes[0])\n",
    "sns.histplot(df_cub['target'], bins=20, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386f4d6",
   "metadata": {
    "id": "8386f4d6"
   },
   "outputs": [],
   "source": [
    "df_cub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6f1af",
   "metadata": {
    "id": "2dc6f1af"
   },
   "outputs": [],
   "source": [
    "df_cub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd2075",
   "metadata": {
    "id": "57bd2075"
   },
   "outputs": [],
   "source": [
    "# Conteo de valores perdidos/faltantes  \n",
    "df_cub.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147dc31c",
   "metadata": {
    "id": "147dc31c"
   },
   "outputs": [],
   "source": [
    "# Consultamos los registros que tienen algún valor nulo\n",
    "df_cub.drop(df_cub.dropna().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bf0cc",
   "metadata": {
    "id": "9a0bf0cc"
   },
   "source": [
    "### Reducción de Variables (___Dimensionality Reduction___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08a016",
   "metadata": {
    "id": "ad08a016",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.1`**\n",
    "\n",
    "Para conseguir un dataset con una dimensión reducidad, aplica la técnica de **Selección de variables basada en árbol de decisión** mediante las importancias de cada variable (**`Decision Trees Importances`**):\n",
    "\n",
    "- Filtra el tablón para quedarnos solamente con **las variables que aglutinan hasta el `95%` de la información** que se requiere para estimar la variable objetivo.\n",
    "- `random_state=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edcd5e-98c2-4aaf-8d51-da5caf540cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Dividimos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_cub.drop('target', axis=1),dataset_cub['target'],test_size=0.2, \n",
    "random_state=100)\n",
    "\n",
    "#Entrenamos un modelo de árbol de decisión y obtenemos la importancia de cada variable\n",
    "model = DecisionTreeRegressor(random_state=100)\n",
    "model.fit(X_train, y_train)\n",
    "importancias = model.feature_importances_\n",
    "\n",
    "#Ordenamos las variables de mayor a menor importancia:\n",
    "indices = importancias.argsort()[::-1]\n",
    "\n",
    "#Calculamos la suma acumulada de la importancia de cada variable\n",
    "importancias_acumulativas = np.cumsum(importancias[indices])\n",
    "\n",
    "#Seleccionamos las variables que aglutinan hasta el 95% de la información necesaria para estimar la variable objetivo\n",
    "selected_indices = np.where(importancias_acumulativas <= 0.95)[0]\n",
    "selected_columns = X_train.columns[indices[selected_indices]]\n",
    "selected_columns\n",
    "\n",
    "#Tendremos una lista con los nombres de las variables seleccionadas.\n",
    "#Podemos utilizar esta lista para filtrar el conjunto de datos original y quedarnos solo con las variables seleccionadas\n",
    "data_selected = dataset_cub[selected_columns]\n",
    "data_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25620659",
   "metadata": {
    "id": "25620659"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece0b60",
   "metadata": {
    "id": "4ece0b60"
   },
   "source": [
    "### **`Ejercicio 18.2`**\n",
    "\n",
    "Después de filtrar el dataset vamos a plantear un **problema de clasificación** para conseguir un **clasificador de la cubierta forestal** en basea a las **variables cartográficas**.:  \n",
    "\n",
    "**`18.2.1`** Genera una gráfica para visualizar la distribución de las variables del datset en conjunto. Analiza dicha gráfica y explica si hay una necesidad de normalizar los datos.  \n",
    "\n",
    "**`18.2.2`** Normaliza todas las variables del dataset a una escala estándar. Para ello puedes realizar estas transformaciones:\n",
    "\n",
    "- LLevar las variables de entrada a una escala de `0` a `1`\n",
    "- Convertir la variable objetivo en valores numéricos **entre 0 y el número de clases menos 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2855c3-4aa7-4a9f-9b18-44b09a9765ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.2.1\n",
    "#Para visualizar la distribución de las variables del dataset en conjunto podemos utilizar un diagrama de caja\n",
    "sns.boxplot(data=data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ba39d-022e-4f90-8ca1-def8a516ae05",
   "metadata": {},
   "source": [
    "Vemos que las distribuciones son muy diferentes entre sí, por lo que sí necesario normalizar los datos para evitar que algunas variables tengan más peso que otras en el modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1a7d0-605d-4517-8ee9-2aeb79bc176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.2.2\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Separar las variables de entrada y la variable objetivo\n",
    "X = dataset_cub.drop('target', axis=1)\n",
    "y = dataset_cub['target']\n",
    "\n",
    "# Normalizar las variables de entrada a una escala de 0 a 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convertir la variable objetivo en valores numéricos entre 0 y el número de clases menos 1\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e22253",
   "metadata": {
    "id": "21e22253"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764f772",
   "metadata": {
    "id": "3764f772",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.3`**\n",
    "\n",
    "Después de estandarizar los datos procedemos a crear el **primer clasificador**:  \n",
    "\n",
    "**`18.3.1`** Divide el datset en _training_ y en _test_:\n",
    "- Guarda el `20%` de los datos para testeo.\n",
    "- `random_state=100`  \n",
    "\n",
    "**`18.3.2`** Entrena un modelo de **regresión logística**:\n",
    "- Número máximo de iteraciones igual a `1000`\n",
    "- `random_state=100`\n",
    "\n",
    "**`18.3.3`** Calcula diferentes métricas para evaluar este modelo y analiza su rendimiendo.\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6bed3-163b-45e2-b8cc-7ba6c04ce955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.3.1\n",
    "\n",
    "# Separar las variables de entrada y la variable objetivo\n",
    "X = dataset_cub.drop('target', axis=1)\n",
    "y = dataset_cub['target']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bdfda-16c0-47ef-8456-8bf8293ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.3.2\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Crear un objeto de regresión logística\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=100)\n",
    "\n",
    "# Entrenar el modelo en el conjunto de entrenamiento\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93638275-94e5-467a-ab09-ef073d46ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.3.3\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "#Un valor alto de Accuracy y F1-score indica que el modelo tiene un buen desempeño en el conjunto de prueba.\n",
    "#La Classification Report y la Confusion Matrix nos proporcionan información adicional sobre el desempeño del modelo en cada clase.\n",
    "\n",
    "# Predecir las clases en el conjunto de prueba\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calcular la Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calcular el F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Imprimir el Classification Report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Imprimir la Confusion Matrix\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160af786-2d1c-4770-973d-4f537d0b9321",
   "metadata": {},
   "source": [
    "Su rendimiento es del 68%, por lo que tampoco es muy alto pero no está mal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68459810",
   "metadata": {
    "id": "68459810"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181a3c8",
   "metadata": {
    "id": "1181a3c8",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.4`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **árboles de decisión**:  \n",
    "\n",
    "**`18.4.1`** Entrena un modelo tipo **Decision Tree Classifire** y calcula las métricas correspondientes para analizar su rendimiento en comparación con el modelo anterior:\n",
    "- `random_state=100`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.4.2`** Saca la curva de complejidad del modelo _Decision Tree_ (**Model Complexity Curve**) y crea un nuevo clasificador **con el valor óptimo de la profundidad del árbol** según esta gráfica. Después saca las métricas correspondiente y analiza el rendimiento del modelo en comparación de los anteriores.\n",
    "- `random_state=100`\n",
    "- rango de profundidades:  de `2` a `30` __inclusive__\n",
    "\n",
    "**`18.4.3`** Saca la gráfica de el *Learning Curve* para estos modelos, definiendo y aplicando una función que toma el valor del hiperparámetro como su entrada y dibuja la evolución del rendimiento del modelo para el conjunto de training y de test. Explica si este último modelo tiene preferencia o no, comparando con modelos anteriores.\n",
    "- `random_state=100`\n",
    "- (*Sugerencia*: No incluya más de 10 puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80e76d-259e-44b9-91f9-84e20b1e0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.4.1\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Crear un modelo de árbol de decisión\n",
    "dtc = DecisionTreeClassifier(random_state=100)\n",
    "\n",
    "# Entrenar el modelo en el conjunto de entrenamiento\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las clases en el conjunto de prueba\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calcular la Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calcular el F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Imprimir el Classification Report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# Imprimir la Confusion Matrix\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58391764-84c7-4145-a21a-1e6a242297c0",
   "metadata": {},
   "source": [
    "Vemos que si entrenamos un modelo tipo Decision Tree Classifire, tenemos un rendimiento del 94%, por lo que es mucho mayor y mejor que cuando lo hicimos con el modelo de regresión logística.\n",
    "\n",
    "En general, los modelos de árbol de decisión tienen la ventaja de ser más interpretables y explicables que los modelos de regresión logística. Sin embargo, pueden ser propensos al sobreajuste y pueden no generalizar bien a nuevos conjuntos de datos.\n",
    "\n",
    "Igualmente, debemos elegir el modelo que tenga un mejor rendimiento en el conjunto de prueba y que sea más adecuado para el problema que estamos intentando resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f21a19-b500-4dd2-9f0a-32275f9849a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.4.2\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Crear un modelo de árbol de decisión\n",
    "dtc = DecisionTreeClassifier(random_state=100)\n",
    "\n",
    "# Definir el rango de profundidades\n",
    "depth_range = range(2, 31)\n",
    "\n",
    "# Calcular la precisión en el conjunto de entrenamiento y prueba para cada profundidad\n",
    "train_scores, test_scores = validation_curve(dtc, X_train, y_train,param_name=\"max_depth\",param_range=depth_range,\n",
    "                                             cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Calcular la media y la desviación estándar de la precisión en cada profundidad\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar la curva de complejidad\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Model Complexity Curve')\n",
    "plt.plot(depth_range, train_mean, label='Training Score', color='blue')\n",
    "plt.fill_between(depth_range, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
    "plt.plot(depth_range, test_mean, label='Cross-Validation Score', color='green')\n",
    "plt.fill_between(depth_range, test_mean - test_std, test_mean + test_std, alpha=0.15, color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb937d32-1e19-4900-b752-abeabef58af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un nuevo modelo de árbol de decisión con la profundidad óptima y evaluamos su rendimiento en el conjunto de prueba\n",
    "\n",
    "# Crear modelo de árbol de decisión con profundidad óptima\n",
    "optimal_depth = 11\n",
    "dtc = DecisionTreeClassifier(max_depth=optimal_depth, random_state=100)\n",
    "\n",
    "# Entrenar modelo con datos de entrenamiento\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en datos de prueba\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calcular métricas de evaluación del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score (weighted):\", f1)\n",
    "print(\"Classification report:\")\n",
    "print(class_report)\n",
    "print(\"Confusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704db96b-666c-4f41-969d-8216821cc303",
   "metadata": {},
   "source": [
    "Ahora tenemos un rendimiento del 79%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579848a3-4a9a-430a-9186-c9b9f7d95124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.4.3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "                                                            random_state=100)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Generar un conjunto de datos de ejemplo\n",
    "X, y = make_classification(n_samples=5000, n_features=10, n_classes=2, random_state=100)\n",
    "\n",
    "# Definir un modelo Decision Tree Classifier\n",
    "estimator = DecisionTreeClassifier(random_state=100)\n",
    "\n",
    "# Generar la gráfica del Learning Curve para el modelo\n",
    "plot_learning_curve(estimator, \"Learning Curve (Decision Tree Classifier)\", X, y, cv=5, n_jobs=-1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc96a0e",
   "metadata": {
    "id": "2bc96a0e"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acde38f",
   "metadata": {
    "id": "5acde38f",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.5`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **bosques aleatorios**:  \n",
    "\n",
    "**`18.5.1`** Entrena un modelo tipo **Random Forest Classifire** y calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- `random_state=100`\n",
    "\n",
    "**`18.5.2`** Consulta la profundidad de todos los árboles del bosque creado en el paso anterior y calcula la mediana de este parámetro.\n",
    "\n",
    "**`18.5.3`** Saca las curvas de complejidad del modelo _Random Forest_ (**Model Complexity Curve**) y crea un nuevo clasificador **con los valores óptimos** analizados dentro de los rangos indicados para cada hiperparámeto. Después crea un modelo con estos parámetros \"óptimos\" y saca las métricas correspondientes para analizar el rendimiento del modelo en comparación con los anteriores.\n",
    "- `random_state=100`\n",
    "- define un rango con funciones de _numpy_ para considerar estos números de árboles: `[200, 250, 300, 350, 400]`  \n",
    "- rango de profundidades:  de `20` a `40` __inclusive__ en pasos de 2 en 2.\n",
    "- considera estas opciones para _max_features_ : `[\"auto\", \"log2\", None]`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "- **OOB** (out-of-bag score)\n",
    "\n",
    "**`18.5.4`** Saca la gráfica del *Learning Curve* para estos modelos, definiendo y aplicando una función que toma el valor de los hiperparámetros analizados como su entrada y dibuja la evolución del rendimiento del modelo para el conjunto de training y de test. Explica si este último modelo tiene preferencia o no, comparando con modelos anteriores.\n",
    "- `random_state=100`\n",
    "- (*Sugerencia*: No incluya más de 10 puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d63e3-cebc-42d8-aee2-ebfdadf878f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.5.1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest=RandomForestClassifier(random_state=100)\n",
    "random_forest.fit(X_train,y_train)\n",
    "\n",
    "forest_predictions = random_forest.predict(X_test)\n",
    "print(accuracy_score(y_test, forest_predictions))\n",
    "\n",
    "print(f1_score(y_test, forest_predictions,average='weighted'))\n",
    "\n",
    "print(classification_report(y_test, forest_predictions,zero_division=0))\n",
    "\n",
    "print(confusion_matrix(y_test, forest_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450acfd1-eca7-41fc-95ef-df66be523f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.5.2\n",
    "# Accedemos a la lista de árboles en el bosque\n",
    "trees = rf.estimators_\n",
    "\n",
    "# Creamos una lista de las profundidades de cada árbol\n",
    "depths = [tree.tree_.max_depth for tree in trees]\n",
    "\n",
    "# Calculamos la mediana de las profundidades\n",
    "median_depth = np.median(depths)\n",
    "\n",
    "# Imprimimos la mediana de las profundidades\n",
    "print(\"Median tree depth:\", median_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760f65f-6d85-4f79-ba36-bb73619b8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.5.3\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_complexity(X_train, y_train, X_test, y_test, n_estimators_range, max_depth_range, max_features_options):\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    train_f1 = []\n",
    "    test_f1 = []\n",
    "    oob_scores = []\n",
    "    for n_estimators in n_estimators_range:\n",
    "        for max_depth in max_depth_range:\n",
    "            for max_features in max_features_options:\n",
    "                rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, oob_score=True, random_state=100)\n",
    "                rf.fit(X_train, y_train)\n",
    "                train_accuracy.append(accuracy_score(y_train, rf.predict(X_train)))\n",
    "                test_accuracy.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "                train_f1.append(f1_score(y_train, rf.predict(X_train), average='weighted'))\n",
    "                test_f1.append(f1_score(y_test, rf.predict(X_test), average='weighted'))\n",
    "                oob_scores.append(rf.oob_score_)\n",
    "    \n",
    "    # Plotting Accuracy scores\n",
    "    train_accuracy = np.array(train_accuracy).reshape(len(n_estimators_range), len(max_depth_range), len(max_features_options))\n",
    "    test_accuracy = np.array(test_accuracy).reshape(len(n_estimators_range), len(max_depth_range), len(max_features_options))\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(max_features_options)):\n",
    "        ax.plot(n_estimators_range, test_accuracy[:, :, i], label='Max Features: ' + str(max_features_options[i]))\n",
    "    ax.set_xlabel('Number of Trees')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting F1 scores\n",
    "    train_f1 = np.array(train_f1).reshape(len(n_estimators_range), len(max_depth_range), len(max_features_options))\n",
    "    test_f1 = np.array(test_f1).reshape(len(n_estimators_range), len(max_depth_range), len(max_features_options))\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(max_features_options)):\n",
    "        ax.plot(n_estimators_range, test_f1[:, :, i], label='Max Features: ' + str(max_features_options[i]))\n",
    "    ax.set_xlabel('Number of Trees')\n",
    "    ax.set_ylabel('F1 Score (weighted)')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting OOB scores\n",
    "    oob_scores = np.array(oob_scores).reshape(len(n_estimators_range), len(max_depth_range), len(max_features_options))\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(max_features_options)):\n",
    "        ax.plot(n_estimators_range, oob_scores[:, :, i], label='Max Features: ' + str(max_features_options[i]))\n",
    "    ax.set_xlabel('Number of Trees')\n",
    "    ax.set_ylabel('OOB Score')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a5913-4cd2-451b-8209-e2f2c03687e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.5.4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "                                                            random_state=100)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Generar un conjunto de datos de ejemplo\n",
    "X, y = make_classification(n_samples=5000, n_features=10, n_classes=2, random_state=100)\n",
    "\n",
    "# Definir un modelo Random Forest Classifier con diferentes hiperparámetros\n",
    "estimator1 = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=100)\n",
    "estimator2 = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=100)\n",
    "estimator3 = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=100)\n",
    "\n",
    "# Generar la gráfica del Learning Curve para los modelos\n",
    "plot_learning_curve(estimator1, \"Learning Curve (Random Forest Classifier, n_estimators=10, max_depth=5)\", X, y, cv=5, n_jobs=-1)\n",
    "plot_learning_curve(estimator2, \"Learning Curve (Random Forest Classifier, n_estimators=50, max_depth=10)\", X, y, cv=5, n_jobs=-1)\n",
    "plot_learning_curve(estimator3, \"Learning Curve (Random Forest Classifier, n_estimators=100, max_depth=20)\", X, y, cv=5, n_jobs=-1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb23696",
   "metadata": {
    "id": "7cb23696"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7fa6f",
   "metadata": {
    "id": "3ed7fa6f",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.6`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **Gradient Boosting**:  \n",
    "\n",
    "**`18.6.1`** Entrena un modelo tipo **XGBoost Classifire** y calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- `random_state=100`\n",
    "\n",
    "**`18.6.2`** Consulta el número y la profundidad máxima de los árboles del bosque creado en el paso anterior.\n",
    "\n",
    "**`18.6.3`** Saca las curvas de complejidad del modelo _XGBClassifier_ (**Model Complexity Curve**) y crea un nuevo clasificador **con los valores óptimos** analizados dentro de los rangos indicados para cada hiperparámeto. Después crea un modelo con estos parámetros \"óptimos\" y saca las métricas correspondientes para analizar el rendimiento del modelo en comparación con los anteriores.\n",
    "- `random_state=100`\n",
    "- define un rango con funciones de _numpy_ para considerar estos números de árboles: `[100, 200, 300, 400, 500]`  \n",
    "- rango de profundidades:  de `6` a `20` __inclusive__ en pasos de 2 en 2.\n",
    "- valores a considerar para el *`learning_rate`*: `[0.01, 0.1, 0.3, 0.5]`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.6.4`** Saca la gráfica del *Learning Curve* para estos modelos, definiendo y aplicando una función que toma el valor de los hiperparámetros analizados como su entrada y dibuja la evolución del rendimiento del modelo para el conjunto de training y de test. Explica si este último modelo tiene preferencia o no, comparando con modelos anteriores.\n",
    "- `random_state=100`\n",
    "- (*Sugerencia*: No incluya más de 10 puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fa679-47a4-4496-a0ce-37f79f9eaea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850eb809-1dfc-410e-bc29-002fec3064ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.6.1\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "xgb_model = XGBClassifier(random_state=100)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción en los conjuntos de training y test\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Cálculo de las métricas de evaluación\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy en training set:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Accuracy en test set:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"F1-score en training set:\", f1_score(y_train, y_pred_train, average='weighted'))\n",
    "print(\"F1-score en test set:\", f1_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"Classification report en test set:\\n\", classification_report(y_test, y_pred_test, zero_division=0))\n",
    "print(\"Confusion matrix en test set:\\n\", confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986ac00-a108-4269-aaaa-0948b8cf03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.6.2\n",
    "#El modelo XGBoost que hemos entrenado no utiliza un bosque de árboles sino un solo árbol de decisión, por lo que no hay un número máximo de árboles que haya que consultar.\n",
    "#Sin embargo, sí podemos consultar la profundidad máxima del árbol generado por el modelo\n",
    "max_depth = xgb_model.max_depth\n",
    "print(\"Profundidad máxima del árbol: \", max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103eb292-49d8-4e03-86e0-675bce71ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.6.3\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Definir el rango de profundidades\n",
    "depth_range = range(2, 31)\n",
    "\n",
    "# Calcular la precisión en el conjunto de entrenamiento y prueba para cada profundidad\n",
    "train_scores, test_scores = validation_curve(XGBoost, X_train, y_train,param_name=\"max_depth\",param_range=depth_range,\n",
    "                                             cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Calcular la media y la desviación estándar de la precisión en cada profundidad\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar la curva de complejidad\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Model Complexity Curve')\n",
    "plt.plot(depth_range, train_mean, label='Training Score', color='blue')\n",
    "plt.fill_between(depth_range, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')\n",
    "plt.plot(depth_range, test_mean, label='Cross-Validation Score', color='green')\n",
    "plt.fill_between(depth_range, test_mean - test_std, test_mean + test_std, alpha=0.15, color='green')\n",
    "plt.legend()\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedda97-e91c-4b58-8a5b-3c8eb4daeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.6.4\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir los valores de los hiperparámetros\n",
    "n_estimators = 400\n",
    "max_depth = 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Crear el modelo con los hiperparámetros definidos\n",
    "model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=100)\n",
    "\n",
    "# Definir los tamaños de muestra a utilizar\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Obtener los scores de entrenamiento y validación para cada tamaño de muestra\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calcular la media y la desviación estándar de los scores de entrenamiento y validación\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Graficar la curva de aprendizaje\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "\n",
    "# Dibujar una banda sombreada que muestra la desviación estándar\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "\n",
    "# Definir los detalles de la gráfica\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76362b0b",
   "metadata": {
    "id": "76362b0b"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5bb8ab",
   "metadata": {
    "id": "bf5bb8ab",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.7`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **métodos Bayesianos**:  \n",
    "\n",
    "**`18.7.1`** Entrena un modelo para cada tipo de algoritmos Bayesianos y calcula las métricas correspondientes para analizar sus rendimientos en comparación con los modelos anteriores:\n",
    "- `GaussianNB`\n",
    "- `MultinomialNB`\n",
    "- `ComplementNB`\n",
    "- `BernoulliNB`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.7.2`** Saca la gráfica del *Learning Curve* para el modelo `GaussianNB` y explica si este modelo sufre de un posible \"_Overfitting_\" o \"_Underfitting_\" comparando con modelos anteriores.\n",
    "- `random_state=100`\n",
    "- (*Sugerencia*: No incluya más de 10 puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ae2bc-148d-468c-9027-b522f81c0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.7.1\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "print(\"GaussianNB metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_gnb, average='weighted'))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred_gnb, zero_division=0))\n",
    "\n",
    "# MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_mnb, average='weighted'))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred_mnb, zero_division=0))\n",
    "\n",
    "# ComplementNB\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "y_pred_cnb = cnb.predict(X_test)\n",
    "\n",
    "print(\"ComplementNB metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_cnb))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_cnb, average='weighted'))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred_cnb, zero_division=0))\n",
    "\n",
    "# BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "\n",
    "print(\"BernoulliNB metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_bnb, average='weighted'))\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred_bnb, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d7b21-51d0-4b77-b305-29bd7ca48726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.7.2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(gnb, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), random_state=100)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve - GaussianNB\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66690c81",
   "metadata": {
    "id": "66690c81"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eec61a",
   "metadata": {
    "id": "33eec61a",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.8`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **K vecinos más cercanos**:  \n",
    "\n",
    "**`18.8.1`** Entrena un modelo tipo **K-Nearest Neighbors** con la configuración por defecto y otros dos modelos con `1` y `100` vecinos más cercanos. Calcula las métricas correspondientes para analizar sus rendimientos en comparación con el modelo anteriores:\n",
    "- `random_state=100`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.8.2`** Saca la gráfica del *Learning Curve* para estos modelos, definiendo y aplicando una función que toma el valor del hiperparámetro analizado como su entrada y dibuja la evolución del rendimiento del modelo para el conjunto de training y de test. Explica si este último modelo tiene preferencia o no, comparando con modelos anteriores.\n",
    "- (*Sugerencia*: No incluya más de 5 puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d27a98-3098-48d6-b1eb-ebb386f793eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.8.1\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Escalado de los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelo con configuración por defecto (5 vecinos)\n",
    "knn_default = KNeighborsClassifier()\n",
    "knn_default.fit(X_train_scaled, y_train)\n",
    "y_pred_default = knn_default.predict(X_test_scaled)\n",
    "\n",
    "# Modelo con 1 vecino\n",
    "knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_1.fit(X_train_scaled, y_train)\n",
    "y_pred_1 = knn_1.predict(X_test_scaled)\n",
    "\n",
    "# Modelo con 100 vecinos\n",
    "knn_100 = KNeighborsClassifier(n_neighbors=100)\n",
    "knn_100.fit(X_train_scaled, y_train)\n",
    "y_pred_100 = knn_100.predict(X_test_scaled)\n",
    "\n",
    "# Métricas\n",
    "print(\"Modelo con configuración por defecto:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_default))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_default, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_default, zero_division=0))\n",
    "\n",
    "print(\"Modelo con 1 vecino:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_1))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_1, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_1, zero_division=0))\n",
    "\n",
    "print(\"Modelo con 100 vecinos:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_100))\n",
    "print(\"F1-score (weighted):\", f1_score(y_test, y_pred_100, average='weighted'))\n",
    "print(classification_report(y_test, y_pred_100, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d5420-79e0-4de8-8fa6-8a20e00816d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.8.2\n",
    "#Primero, definimos una lista de los valores a considerar para n_neighbors:\n",
    "n_neighbors_values = [1, 5, 10, 50, 100]\n",
    "\n",
    "#Luego, podemos utilizar un bucle for para iterar sobre esta lista y generar la gráfica para cada valor de n_neighbors. Por ejemplo, para n_neighbors=1:\n",
    "model_knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "plot_learning_curve(model_knn_1, \"KNN (n_neighbors=1)\", X_train, y_train, cv=5, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8950a61",
   "metadata": {
    "id": "b8950a61"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc8786",
   "metadata": {
    "id": "a0fc8786",
    "tags": []
   },
   "source": [
    "### **`Ejercicio 18.9`**\n",
    "\n",
    "Ahora probamos la creación de otros modelo basados en **Redes Neuronales**:  \n",
    "\n",
    "**`18.9.1`** Entrena un modelo tipo **MLPClassifier** y calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- `random_state=100`\n",
    "\n",
    "\n",
    "**`18.9.2`** Entrena otro modelo tipo _MLPClassifier_ indicando los siguientes hiperparámetros y calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- `random_state=100`\n",
    "- `hidden_layer_sizes=(100,200,100)`\n",
    "- Número máximo de iteraciones igual a `10000`\n",
    "- `alpha=1e-5`\n",
    "- `tol=1e-5`\n",
    "\n",
    "**`18.9.3`** Construye con la misma estructura del modelo definido en el paso anterior, una red neuronal profunda (**DNN**-Deep Neural Networks) usando la librería _keras_  realizando las preparaciones y tratamientos necesarias al respecto y considerando los siguientes parámetros. Después, calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- `semilla = 883`\n",
    "- `epochs = 50`\n",
    "- `batch_size=100`\n",
    "- `loss='binary_crossentropy`\n",
    "- `optimizer='Adam'`\n",
    "- `umbral = 0.5`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.9.4`** Crea otra red neuronal profunda usando _keras_ y considerando los siguientes parámetros. Después, calcula las métricas correspondientes para analizar su rendimiento en comparación con los modelos anteriores:\n",
    "- Definir las capas ocultas: \n",
    "   - Una capa densa con **50** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **100** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **200** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **1000** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **200** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **100** neuronas y la misma función de activación que la red anterior\n",
    "   - Una capa densa con **50** neuronas y la misma función de activación que la red anterior\n",
    "      \n",
    "- `semilla = 883`\n",
    "- `epochs = 40`\n",
    "- `batch_size=100`\n",
    "- `loss='binary_crossentropy`\n",
    "- `optimizer='Adam'`\n",
    "- `umbral = 0.5`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "\n",
    "**`18.9.5`** Saca la gráfica del *Learning Curve* para este último modelo y explica si tiene preferencia o no, comparando con modelos anteriores.\n",
    "- `semilla = 883`\n",
    "- `epochs = 40`\n",
    "- `batch_size=8000`\n",
    "- `loss='binary_crossentropy`\n",
    "- `optimizer='Adam'`\n",
    "- `umbral = 0.5`\n",
    "- Accuracy\n",
    "- F1-score `(average='weighted')`\n",
    "- Classification report `(zero_division=0)`\n",
    "- (*Sugerencia*: No incluya más de **5** puntos en el eje horizontal y empieza la gráfica con un mínimo de _1000_ muestras para el modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d14a5-eb3e-4edf-b2ca-06afca2da967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.9.1\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "# Crear el modelo MLPClassifier\n",
    "mlp = MLPClassifier(random_state=100)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones sobre los datos de prueba\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calcular algunas métricas de evaluación\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b587a79-7e96-4551-8cf7-a87a2ef5f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.9.2\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "# Crear el modelo MLPClassifier con los hiperparámetros especificados\n",
    "mlp = MLPClassifier(random_state=100, hidden_layer_sizes=(100,200,100), max_iter=10000, alpha=1e-5, tol=1e-5)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones sobre los datos de prueba\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calcular algunas métricas de evaluación\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ae324-02eb-4007-98fe-7fc9b3e8648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.9.3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=883)\n",
    "\n",
    "# Normalizar los datos\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=100, verbose=0)\n",
    "\n",
    "# Hacer predicciones sobre los datos de prueba\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Calcular algunas métricas de evaluación\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "class_report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "# Imprimir las métricas de evaluación\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9525e-e9a7-47fc-88c3-f0c6b224bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.9.4\n",
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Establecer la semilla\n",
    "np.random.seed(883)\n",
    "tf.random.set_seed(883)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convertir las variables objetivo en arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=100, verbose=0)\n",
    "\n",
    "# Realizar las predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular la precisión\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calcular la medida F1\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Generar el classification report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print('Matriz de confusión:')\n",
    "print(confusion_matrix)\n",
    "print('Precisión:', accuracy)\n",
    "print('F1-score:', f1_score)\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de0e5a-1868-4096-a7e4-a8a7b023fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18.9.5\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "plot_model(history.history, ['accuracy', 'val_accuracy'])\n",
    "plt.title('Learning Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
